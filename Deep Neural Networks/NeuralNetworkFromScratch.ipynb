{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Neural Network from scratch**\n",
        "\n",
        "**In this project i will implement Neural Network to recognite handwritten digit**:\n",
        "\n",
        "*   The dataset for this project would be MNIST handwritten digit dataset\n",
        "*   I can not use any frameworks related to deep learning in this project\n",
        "\n"
      ],
      "metadata": {
        "id": "jaXsQj0nG1zm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7_5ol763e-P9"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBrdmwetfM1i",
        "outputId": "9e623677-4e99-40c5-fbf2-0c5d199e3e87"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "KJFg8B-Te-P_"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Projects/Digit Recognition/Dataset/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Projects/Digit Recognition/Dataset/test.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prepare Data**"
      ],
      "metadata": {
        "id": "HVCz_URDIJbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training data in to X and y\n",
        "X = train.iloc[:, 1:].values\n",
        "y = train.iloc[:, 0].values.reshape(-1, 1)\n",
        "\n",
        "# I will also preprocess the test set with respect to trainning set for submission\n",
        "test = test.values.T\n",
        "\n",
        "# Normalizing data\n",
        "X = X / 255.\n",
        "test = test / 255."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tND0qBsgHNN",
        "outputId": "16917e35-e237-4d9b-cf60-fae4c84668e7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42000, 784), (42000, 1), (784, 28000))"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into 2 (Train set and validation set)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size= 0.24, random_state= 42)\n",
        "\n",
        "# Transpose X_train, X_val, y_train, y_val --> We want the dimension to be (n, m) where n are pixels and m are examples\n",
        "X_train, X_val = X_train.T, X_val.T\n",
        "y_train, y_val = y_train.reshape(1, -1), y_val.reshape(1, -1)\n",
        "\n",
        "# Check for the shape\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "d3N_NJ4TeD2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8a0f43-282f-4a79-a3bf-dcebd8a2180c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((784, 31920), (1, 31920), (784, 10080), (1, 10080))"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model development**"
      ],
      "metadata": {
        "id": "QhQAarXEMsGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify layers for Neural Networks\n",
        "n_x, n_h1, n_h2, n_y = X_train.shape[0], 64, 32, 10\n",
        "\n",
        "# Initialize parameters for Neural Networks\n",
        "def initialize_params(n_x, n_h1, n_h2, n_y):\n",
        "    W1 = np.random.randn(n_h1, n_x) * np.sqrt(2. / n_x) # Applying'He Initialization\" initialize method which help gradient descent converge faster\n",
        "    b1 = np.zeros((n_h1, 1))\n",
        "    W2 = np.random.randn(n_h2, n_h1) * np.sqrt(2. / n_h1)\n",
        "    b2 = np.zeros((n_h2, 1))\n",
        "    W3 = np.random.randn(n_y, n_h2) * np.sqrt(2. / n_h2)\n",
        "    b3 = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {'W1': W1, 'b1': b1,   # Store all parameters in a dictionary\n",
        "                  'W2': W2, 'b2': b2,\n",
        "                  'W3': W3, 'b3': b3}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "# I will also write out the activation functions for forward propagation step -> ReLU for hidden layers and softmax for output layer\n",
        "\n",
        "@staticmethod\n",
        "def ReLU(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "@staticmethod\n",
        "def ReLU_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "@staticmethod\n",
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # for numerical stability\n",
        "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "\n",
        "@staticmethod\n",
        "def one_hot_encoded(y, n_y=10):         # We also need to one hot encode Y\n",
        "    one_hot_y = np.zeros((n_y, y.size))\n",
        "    one_hot_y[y, np.arange(y.size)] = 1\n",
        "    return one_hot_y\n",
        "\n",
        "\n",
        "def forward_prop(parameters, X):                # Forward propagation step\n",
        "    W1, b1 = parameters['W1'], parameters['b1']\n",
        "    W2, b2 = parameters['W2'], parameters['b2']\n",
        "    W3, b3 = parameters['W3'], parameters['b3']\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "    cache = {'Z1': Z1, 'A1': A1, # Store all answers in a dictionary\n",
        "             'Z2': Z2, 'A2': A2,\n",
        "             'Z3': Z3, 'A3': A3}\n",
        "\n",
        "    return A3, cache\n",
        "\n",
        "\n",
        "def cross_entropy(A3, y):    # For multi-class classification i will use cross entropy to calculate the cost for each examples\n",
        "    m = A3.shape[1]\n",
        "    cost = - np.sum(y * np.log(A3 + 1e-8)) / m\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "\n",
        "def back_prop(parameters, cache, X, y):          # Back propagation step\n",
        "    m = y.shape[1]\n",
        "    W3, W2 = parameters['W3'], parameters['W2']\n",
        "    Z1, A1 = cache['Z1'], cache['A1']\n",
        "    Z2, A2 = cache['Z2'], cache['A2']\n",
        "    A3 = cache['A3']\n",
        "\n",
        "    one_hot_y = one_hot_encoded(y)\n",
        "\n",
        "    dZ3 = A3 - one_hot_y\n",
        "    dW3 = np.dot(dZ3, A2.T) / m\n",
        "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
        "    dZ2 = np.dot(W3.T, dZ3) * ReLU_derivative(Z2)\n",
        "    dW2 = np.dot(dZ2, A1.T) / m\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "    dZ1 = np.dot(W2.T, dZ2) * ReLU_derivative(Z1)\n",
        "    dW1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "    grads = {'dW3': dW3, 'db3': db3,\n",
        "             'dW2': dW2, 'db2': db2,\n",
        "             'dW1': dW1, 'db1': db1}\n",
        "\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_params(grads, parameters, learning_rate): # After calculate gradients i will use it to update all parameters\n",
        "    for param in parameters:\n",
        "        parameters[param] -= learning_rate * grads['d' + param]\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def DeepNeuralNetworks(X, y, iters, learning_rate, n_x, n_h1, n_h2, n_y): # Now putting all together in one function\n",
        "    parameters = initialize_params(n_x, n_h1, n_h2, n_y)\n",
        "    one_hot_y = one_hot_encoded(y)\n",
        "\n",
        "    for i in range(iters):\n",
        "        A3, cache = forward_prop(parameters, X)\n",
        "        cost = cross_entropy(A3, one_hot_y)\n",
        "        if i % 10 == 0:\n",
        "            print(f'Cost at iteration {i}: {cost:.5f}')\n",
        "        grads = back_prop(parameters, cache, X, y)\n",
        "        parameters = update_params(grads, parameters, learning_rate)\n",
        "    return parameters\n",
        "\n",
        "\n",
        "\n",
        "def predict(updated_parameters, X):  # Define a predict function\n",
        "    predictions, _ = forward_prop(updated_parameters, X)\n",
        "    return np.argmax(predictions, axis= 0)\n",
        "\n",
        "def score(predictions, y): # Define a score function to get the accuracy\n",
        "    score = np.mean(predictions == y)\n",
        "    return score\n",
        "\n",
        "\n",
        "updated_parameters = DeepNeuralNetworks(X_train, y_train, iters=4500, learning_rate=0.075, n_x=n_x, n_h1=n_h1, n_h2=n_h2, n_y=n_y) # Train the model\n",
        "y_preds = predict(updated_parameters, X_train)\n",
        "accuracy = score(y_preds, y_train)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f} %')"
      ],
      "metadata": {
        "id": "QvmDSNnF5Eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba310bfc-30bd-4c17-bc41-768f62b3f1f9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at iteration 0: 2.41395\n",
            "Cost at iteration 10: 2.00804\n",
            "Cost at iteration 20: 1.55639\n",
            "Cost at iteration 30: 1.14564\n",
            "Cost at iteration 40: 0.87345\n",
            "Cost at iteration 50: 0.71885\n",
            "Cost at iteration 60: 0.62423\n",
            "Cost at iteration 70: 0.56072\n",
            "Cost at iteration 80: 0.51508\n",
            "Cost at iteration 90: 0.48060\n",
            "Cost at iteration 100: 0.45356\n",
            "Cost at iteration 110: 0.43172\n",
            "Cost at iteration 120: 0.41368\n",
            "Cost at iteration 130: 0.39849\n",
            "Cost at iteration 140: 0.38546\n",
            "Cost at iteration 150: 0.37414\n",
            "Cost at iteration 160: 0.36421\n",
            "Cost at iteration 170: 0.35541\n",
            "Cost at iteration 180: 0.34751\n",
            "Cost at iteration 190: 0.34036\n",
            "Cost at iteration 200: 0.33385\n",
            "Cost at iteration 210: 0.32787\n",
            "Cost at iteration 220: 0.32236\n",
            "Cost at iteration 230: 0.31726\n",
            "Cost at iteration 240: 0.31252\n",
            "Cost at iteration 250: 0.30809\n",
            "Cost at iteration 260: 0.30393\n",
            "Cost at iteration 270: 0.30002\n",
            "Cost at iteration 280: 0.29632\n",
            "Cost at iteration 290: 0.29281\n",
            "Cost at iteration 300: 0.28948\n",
            "Cost at iteration 310: 0.28630\n",
            "Cost at iteration 320: 0.28324\n",
            "Cost at iteration 330: 0.28032\n",
            "Cost at iteration 340: 0.27752\n",
            "Cost at iteration 350: 0.27483\n",
            "Cost at iteration 360: 0.27224\n",
            "Cost at iteration 370: 0.26973\n",
            "Cost at iteration 380: 0.26730\n",
            "Cost at iteration 390: 0.26496\n",
            "Cost at iteration 400: 0.26268\n",
            "Cost at iteration 410: 0.26047\n",
            "Cost at iteration 420: 0.25834\n",
            "Cost at iteration 430: 0.25626\n",
            "Cost at iteration 440: 0.25424\n",
            "Cost at iteration 450: 0.25227\n",
            "Cost at iteration 460: 0.25035\n",
            "Cost at iteration 470: 0.24848\n",
            "Cost at iteration 480: 0.24665\n",
            "Cost at iteration 490: 0.24486\n",
            "Cost at iteration 500: 0.24311\n",
            "Cost at iteration 510: 0.24139\n",
            "Cost at iteration 520: 0.23971\n",
            "Cost at iteration 530: 0.23807\n",
            "Cost at iteration 540: 0.23646\n",
            "Cost at iteration 550: 0.23488\n",
            "Cost at iteration 560: 0.23333\n",
            "Cost at iteration 570: 0.23181\n",
            "Cost at iteration 580: 0.23031\n",
            "Cost at iteration 590: 0.22884\n",
            "Cost at iteration 600: 0.22739\n",
            "Cost at iteration 610: 0.22598\n",
            "Cost at iteration 620: 0.22459\n",
            "Cost at iteration 630: 0.22322\n",
            "Cost at iteration 640: 0.22188\n",
            "Cost at iteration 650: 0.22055\n",
            "Cost at iteration 660: 0.21924\n",
            "Cost at iteration 670: 0.21795\n",
            "Cost at iteration 680: 0.21668\n",
            "Cost at iteration 690: 0.21542\n",
            "Cost at iteration 700: 0.21418\n",
            "Cost at iteration 710: 0.21295\n",
            "Cost at iteration 720: 0.21173\n",
            "Cost at iteration 730: 0.21053\n",
            "Cost at iteration 740: 0.20934\n",
            "Cost at iteration 750: 0.20817\n",
            "Cost at iteration 760: 0.20702\n",
            "Cost at iteration 770: 0.20588\n",
            "Cost at iteration 780: 0.20475\n",
            "Cost at iteration 790: 0.20364\n",
            "Cost at iteration 800: 0.20254\n",
            "Cost at iteration 810: 0.20146\n",
            "Cost at iteration 820: 0.20039\n",
            "Cost at iteration 830: 0.19932\n",
            "Cost at iteration 840: 0.19827\n",
            "Cost at iteration 850: 0.19723\n",
            "Cost at iteration 860: 0.19620\n",
            "Cost at iteration 870: 0.19518\n",
            "Cost at iteration 880: 0.19418\n",
            "Cost at iteration 890: 0.19318\n",
            "Cost at iteration 900: 0.19219\n",
            "Cost at iteration 910: 0.19121\n",
            "Cost at iteration 920: 0.19023\n",
            "Cost at iteration 930: 0.18926\n",
            "Cost at iteration 940: 0.18830\n",
            "Cost at iteration 950: 0.18734\n",
            "Cost at iteration 960: 0.18639\n",
            "Cost at iteration 970: 0.18545\n",
            "Cost at iteration 980: 0.18451\n",
            "Cost at iteration 990: 0.18359\n",
            "Cost at iteration 1000: 0.18267\n",
            "Cost at iteration 1010: 0.18175\n",
            "Cost at iteration 1020: 0.18084\n",
            "Cost at iteration 1030: 0.17994\n",
            "Cost at iteration 1040: 0.17905\n",
            "Cost at iteration 1050: 0.17817\n",
            "Cost at iteration 1060: 0.17729\n",
            "Cost at iteration 1070: 0.17642\n",
            "Cost at iteration 1080: 0.17556\n",
            "Cost at iteration 1090: 0.17470\n",
            "Cost at iteration 1100: 0.17385\n",
            "Cost at iteration 1110: 0.17301\n",
            "Cost at iteration 1120: 0.17216\n",
            "Cost at iteration 1130: 0.17133\n",
            "Cost at iteration 1140: 0.17051\n",
            "Cost at iteration 1150: 0.16968\n",
            "Cost at iteration 1160: 0.16887\n",
            "Cost at iteration 1170: 0.16806\n",
            "Cost at iteration 1180: 0.16725\n",
            "Cost at iteration 1190: 0.16645\n",
            "Cost at iteration 1200: 0.16565\n",
            "Cost at iteration 1210: 0.16486\n",
            "Cost at iteration 1220: 0.16408\n",
            "Cost at iteration 1230: 0.16330\n",
            "Cost at iteration 1240: 0.16253\n",
            "Cost at iteration 1250: 0.16176\n",
            "Cost at iteration 1260: 0.16100\n",
            "Cost at iteration 1270: 0.16025\n",
            "Cost at iteration 1280: 0.15950\n",
            "Cost at iteration 1290: 0.15875\n",
            "Cost at iteration 1300: 0.15801\n",
            "Cost at iteration 1310: 0.15727\n",
            "Cost at iteration 1320: 0.15654\n",
            "Cost at iteration 1330: 0.15581\n",
            "Cost at iteration 1340: 0.15509\n",
            "Cost at iteration 1350: 0.15437\n",
            "Cost at iteration 1360: 0.15366\n",
            "Cost at iteration 1370: 0.15296\n",
            "Cost at iteration 1380: 0.15226\n",
            "Cost at iteration 1390: 0.15156\n",
            "Cost at iteration 1400: 0.15087\n",
            "Cost at iteration 1410: 0.15018\n",
            "Cost at iteration 1420: 0.14950\n",
            "Cost at iteration 1430: 0.14883\n",
            "Cost at iteration 1440: 0.14815\n",
            "Cost at iteration 1450: 0.14749\n",
            "Cost at iteration 1460: 0.14682\n",
            "Cost at iteration 1470: 0.14616\n",
            "Cost at iteration 1480: 0.14551\n",
            "Cost at iteration 1490: 0.14486\n",
            "Cost at iteration 1500: 0.14422\n",
            "Cost at iteration 1510: 0.14357\n",
            "Cost at iteration 1520: 0.14293\n",
            "Cost at iteration 1530: 0.14230\n",
            "Cost at iteration 1540: 0.14167\n",
            "Cost at iteration 1550: 0.14104\n",
            "Cost at iteration 1560: 0.14041\n",
            "Cost at iteration 1570: 0.13979\n",
            "Cost at iteration 1580: 0.13918\n",
            "Cost at iteration 1590: 0.13856\n",
            "Cost at iteration 1600: 0.13795\n",
            "Cost at iteration 1610: 0.13734\n",
            "Cost at iteration 1620: 0.13673\n",
            "Cost at iteration 1630: 0.13613\n",
            "Cost at iteration 1640: 0.13553\n",
            "Cost at iteration 1650: 0.13493\n",
            "Cost at iteration 1660: 0.13434\n",
            "Cost at iteration 1670: 0.13375\n",
            "Cost at iteration 1680: 0.13316\n",
            "Cost at iteration 1690: 0.13258\n",
            "Cost at iteration 1700: 0.13200\n",
            "Cost at iteration 1710: 0.13142\n",
            "Cost at iteration 1720: 0.13085\n",
            "Cost at iteration 1730: 0.13028\n",
            "Cost at iteration 1740: 0.12971\n",
            "Cost at iteration 1750: 0.12915\n",
            "Cost at iteration 1760: 0.12859\n",
            "Cost at iteration 1770: 0.12803\n",
            "Cost at iteration 1780: 0.12748\n",
            "Cost at iteration 1790: 0.12693\n",
            "Cost at iteration 1800: 0.12638\n",
            "Cost at iteration 1810: 0.12584\n",
            "Cost at iteration 1820: 0.12530\n",
            "Cost at iteration 1830: 0.12476\n",
            "Cost at iteration 1840: 0.12422\n",
            "Cost at iteration 1850: 0.12369\n",
            "Cost at iteration 1860: 0.12316\n",
            "Cost at iteration 1870: 0.12264\n",
            "Cost at iteration 1880: 0.12211\n",
            "Cost at iteration 1890: 0.12159\n",
            "Cost at iteration 1900: 0.12108\n",
            "Cost at iteration 1910: 0.12056\n",
            "Cost at iteration 1920: 0.12005\n",
            "Cost at iteration 1930: 0.11955\n",
            "Cost at iteration 1940: 0.11904\n",
            "Cost at iteration 1950: 0.11854\n",
            "Cost at iteration 1960: 0.11804\n",
            "Cost at iteration 1970: 0.11755\n",
            "Cost at iteration 1980: 0.11706\n",
            "Cost at iteration 1990: 0.11657\n",
            "Cost at iteration 2000: 0.11608\n",
            "Cost at iteration 2010: 0.11560\n",
            "Cost at iteration 2020: 0.11512\n",
            "Cost at iteration 2030: 0.11465\n",
            "Cost at iteration 2040: 0.11417\n",
            "Cost at iteration 2050: 0.11371\n",
            "Cost at iteration 2060: 0.11324\n",
            "Cost at iteration 2070: 0.11278\n",
            "Cost at iteration 2080: 0.11231\n",
            "Cost at iteration 2090: 0.11186\n",
            "Cost at iteration 2100: 0.11140\n",
            "Cost at iteration 2110: 0.11095\n",
            "Cost at iteration 2120: 0.11050\n",
            "Cost at iteration 2130: 0.11006\n",
            "Cost at iteration 2140: 0.10961\n",
            "Cost at iteration 2150: 0.10917\n",
            "Cost at iteration 2160: 0.10873\n",
            "Cost at iteration 2170: 0.10830\n",
            "Cost at iteration 2180: 0.10787\n",
            "Cost at iteration 2190: 0.10744\n",
            "Cost at iteration 2200: 0.10701\n",
            "Cost at iteration 2210: 0.10658\n",
            "Cost at iteration 2220: 0.10616\n",
            "Cost at iteration 2230: 0.10574\n",
            "Cost at iteration 2240: 0.10532\n",
            "Cost at iteration 2250: 0.10490\n",
            "Cost at iteration 2260: 0.10449\n",
            "Cost at iteration 2270: 0.10408\n",
            "Cost at iteration 2280: 0.10367\n",
            "Cost at iteration 2290: 0.10326\n",
            "Cost at iteration 2300: 0.10286\n",
            "Cost at iteration 2310: 0.10246\n",
            "Cost at iteration 2320: 0.10206\n",
            "Cost at iteration 2330: 0.10166\n",
            "Cost at iteration 2340: 0.10127\n",
            "Cost at iteration 2350: 0.10087\n",
            "Cost at iteration 2360: 0.10048\n",
            "Cost at iteration 2370: 0.10009\n",
            "Cost at iteration 2380: 0.09971\n",
            "Cost at iteration 2390: 0.09932\n",
            "Cost at iteration 2400: 0.09894\n",
            "Cost at iteration 2410: 0.09856\n",
            "Cost at iteration 2420: 0.09819\n",
            "Cost at iteration 2430: 0.09781\n",
            "Cost at iteration 2440: 0.09744\n",
            "Cost at iteration 2450: 0.09707\n",
            "Cost at iteration 2460: 0.09670\n",
            "Cost at iteration 2470: 0.09634\n",
            "Cost at iteration 2480: 0.09597\n",
            "Cost at iteration 2490: 0.09561\n",
            "Cost at iteration 2500: 0.09525\n",
            "Cost at iteration 2510: 0.09489\n",
            "Cost at iteration 2520: 0.09454\n",
            "Cost at iteration 2530: 0.09419\n",
            "Cost at iteration 2540: 0.09384\n",
            "Cost at iteration 2550: 0.09349\n",
            "Cost at iteration 2560: 0.09314\n",
            "Cost at iteration 2570: 0.09280\n",
            "Cost at iteration 2580: 0.09246\n",
            "Cost at iteration 2590: 0.09212\n",
            "Cost at iteration 2600: 0.09178\n",
            "Cost at iteration 2610: 0.09145\n",
            "Cost at iteration 2620: 0.09111\n",
            "Cost at iteration 2630: 0.09078\n",
            "Cost at iteration 2640: 0.09045\n",
            "Cost at iteration 2650: 0.09012\n",
            "Cost at iteration 2660: 0.08980\n",
            "Cost at iteration 2670: 0.08947\n",
            "Cost at iteration 2680: 0.08915\n",
            "Cost at iteration 2690: 0.08883\n",
            "Cost at iteration 2700: 0.08851\n",
            "Cost at iteration 2710: 0.08819\n",
            "Cost at iteration 2720: 0.08787\n",
            "Cost at iteration 2730: 0.08755\n",
            "Cost at iteration 2740: 0.08724\n",
            "Cost at iteration 2750: 0.08693\n",
            "Cost at iteration 2760: 0.08662\n",
            "Cost at iteration 2770: 0.08631\n",
            "Cost at iteration 2780: 0.08600\n",
            "Cost at iteration 2790: 0.08570\n",
            "Cost at iteration 2800: 0.08539\n",
            "Cost at iteration 2810: 0.08509\n",
            "Cost at iteration 2820: 0.08479\n",
            "Cost at iteration 2830: 0.08449\n",
            "Cost at iteration 2840: 0.08419\n",
            "Cost at iteration 2850: 0.08389\n",
            "Cost at iteration 2860: 0.08360\n",
            "Cost at iteration 2870: 0.08330\n",
            "Cost at iteration 2880: 0.08301\n",
            "Cost at iteration 2890: 0.08272\n",
            "Cost at iteration 2900: 0.08243\n",
            "Cost at iteration 2910: 0.08214\n",
            "Cost at iteration 2920: 0.08185\n",
            "Cost at iteration 2930: 0.08157\n",
            "Cost at iteration 2940: 0.08128\n",
            "Cost at iteration 2950: 0.08100\n",
            "Cost at iteration 2960: 0.08072\n",
            "Cost at iteration 2970: 0.08044\n",
            "Cost at iteration 2980: 0.08016\n",
            "Cost at iteration 2990: 0.07989\n",
            "Cost at iteration 3000: 0.07961\n",
            "Cost at iteration 3010: 0.07934\n",
            "Cost at iteration 3020: 0.07906\n",
            "Cost at iteration 3030: 0.07879\n",
            "Cost at iteration 3040: 0.07852\n",
            "Cost at iteration 3050: 0.07825\n",
            "Cost at iteration 3060: 0.07798\n",
            "Cost at iteration 3070: 0.07771\n",
            "Cost at iteration 3080: 0.07745\n",
            "Cost at iteration 3090: 0.07718\n",
            "Cost at iteration 3100: 0.07692\n",
            "Cost at iteration 3110: 0.07666\n",
            "Cost at iteration 3120: 0.07639\n",
            "Cost at iteration 3130: 0.07613\n",
            "Cost at iteration 3140: 0.07587\n",
            "Cost at iteration 3150: 0.07562\n",
            "Cost at iteration 3160: 0.07536\n",
            "Cost at iteration 3170: 0.07510\n",
            "Cost at iteration 3180: 0.07485\n",
            "Cost at iteration 3190: 0.07459\n",
            "Cost at iteration 3200: 0.07434\n",
            "Cost at iteration 3210: 0.07409\n",
            "Cost at iteration 3220: 0.07384\n",
            "Cost at iteration 3230: 0.07359\n",
            "Cost at iteration 3240: 0.07334\n",
            "Cost at iteration 3250: 0.07309\n",
            "Cost at iteration 3260: 0.07284\n",
            "Cost at iteration 3270: 0.07260\n",
            "Cost at iteration 3280: 0.07235\n",
            "Cost at iteration 3290: 0.07211\n",
            "Cost at iteration 3300: 0.07187\n",
            "Cost at iteration 3310: 0.07163\n",
            "Cost at iteration 3320: 0.07139\n",
            "Cost at iteration 3330: 0.07115\n",
            "Cost at iteration 3340: 0.07091\n",
            "Cost at iteration 3350: 0.07068\n",
            "Cost at iteration 3360: 0.07044\n",
            "Cost at iteration 3370: 0.07021\n",
            "Cost at iteration 3380: 0.06997\n",
            "Cost at iteration 3390: 0.06974\n",
            "Cost at iteration 3400: 0.06951\n",
            "Cost at iteration 3410: 0.06928\n",
            "Cost at iteration 3420: 0.06905\n",
            "Cost at iteration 3430: 0.06882\n",
            "Cost at iteration 3440: 0.06859\n",
            "Cost at iteration 3450: 0.06837\n",
            "Cost at iteration 3460: 0.06814\n",
            "Cost at iteration 3470: 0.06792\n",
            "Cost at iteration 3480: 0.06769\n",
            "Cost at iteration 3490: 0.06747\n",
            "Cost at iteration 3500: 0.06725\n",
            "Cost at iteration 3510: 0.06703\n",
            "Cost at iteration 3520: 0.06681\n",
            "Cost at iteration 3530: 0.06659\n",
            "Cost at iteration 3540: 0.06638\n",
            "Cost at iteration 3550: 0.06616\n",
            "Cost at iteration 3560: 0.06595\n",
            "Cost at iteration 3570: 0.06574\n",
            "Cost at iteration 3580: 0.06552\n",
            "Cost at iteration 3590: 0.06531\n",
            "Cost at iteration 3600: 0.06510\n",
            "Cost at iteration 3610: 0.06489\n",
            "Cost at iteration 3620: 0.06468\n",
            "Cost at iteration 3630: 0.06448\n",
            "Cost at iteration 3640: 0.06427\n",
            "Cost at iteration 3650: 0.06406\n",
            "Cost at iteration 3660: 0.06386\n",
            "Cost at iteration 3670: 0.06366\n",
            "Cost at iteration 3680: 0.06345\n",
            "Cost at iteration 3690: 0.06325\n",
            "Cost at iteration 3700: 0.06305\n",
            "Cost at iteration 3710: 0.06285\n",
            "Cost at iteration 3720: 0.06265\n",
            "Cost at iteration 3730: 0.06245\n",
            "Cost at iteration 3740: 0.06225\n",
            "Cost at iteration 3750: 0.06206\n",
            "Cost at iteration 3760: 0.06186\n",
            "Cost at iteration 3770: 0.06166\n",
            "Cost at iteration 3780: 0.06147\n",
            "Cost at iteration 3790: 0.06127\n",
            "Cost at iteration 3800: 0.06108\n",
            "Cost at iteration 3810: 0.06089\n",
            "Cost at iteration 3820: 0.06070\n",
            "Cost at iteration 3830: 0.06051\n",
            "Cost at iteration 3840: 0.06032\n",
            "Cost at iteration 3850: 0.06013\n",
            "Cost at iteration 3860: 0.05994\n",
            "Cost at iteration 3870: 0.05975\n",
            "Cost at iteration 3880: 0.05956\n",
            "Cost at iteration 3890: 0.05938\n",
            "Cost at iteration 3900: 0.05919\n",
            "Cost at iteration 3910: 0.05901\n",
            "Cost at iteration 3920: 0.05882\n",
            "Cost at iteration 3930: 0.05864\n",
            "Cost at iteration 3940: 0.05846\n",
            "Cost at iteration 3950: 0.05828\n",
            "Cost at iteration 3960: 0.05810\n",
            "Cost at iteration 3970: 0.05792\n",
            "Cost at iteration 3980: 0.05774\n",
            "Cost at iteration 3990: 0.05756\n",
            "Cost at iteration 4000: 0.05739\n",
            "Cost at iteration 4010: 0.05721\n",
            "Cost at iteration 4020: 0.05703\n",
            "Cost at iteration 4030: 0.05686\n",
            "Cost at iteration 4040: 0.05668\n",
            "Cost at iteration 4050: 0.05651\n",
            "Cost at iteration 4060: 0.05633\n",
            "Cost at iteration 4070: 0.05616\n",
            "Cost at iteration 4080: 0.05599\n",
            "Cost at iteration 4090: 0.05582\n",
            "Cost at iteration 4100: 0.05565\n",
            "Cost at iteration 4110: 0.05548\n",
            "Cost at iteration 4120: 0.05531\n",
            "Cost at iteration 4130: 0.05514\n",
            "Cost at iteration 4140: 0.05497\n",
            "Cost at iteration 4150: 0.05480\n",
            "Cost at iteration 4160: 0.05464\n",
            "Cost at iteration 4170: 0.05447\n",
            "Cost at iteration 4180: 0.05431\n",
            "Cost at iteration 4190: 0.05414\n",
            "Cost at iteration 4200: 0.05398\n",
            "Cost at iteration 4210: 0.05381\n",
            "Cost at iteration 4220: 0.05365\n",
            "Cost at iteration 4230: 0.05349\n",
            "Cost at iteration 4240: 0.05333\n",
            "Cost at iteration 4250: 0.05317\n",
            "Cost at iteration 4260: 0.05301\n",
            "Cost at iteration 4270: 0.05285\n",
            "Cost at iteration 4280: 0.05269\n",
            "Cost at iteration 4290: 0.05253\n",
            "Cost at iteration 4300: 0.05238\n",
            "Cost at iteration 4310: 0.05222\n",
            "Cost at iteration 4320: 0.05206\n",
            "Cost at iteration 4330: 0.05191\n",
            "Cost at iteration 4340: 0.05175\n",
            "Cost at iteration 4350: 0.05160\n",
            "Cost at iteration 4360: 0.05145\n",
            "Cost at iteration 4370: 0.05129\n",
            "Cost at iteration 4380: 0.05114\n",
            "Cost at iteration 4390: 0.05099\n",
            "Cost at iteration 4400: 0.05084\n",
            "Cost at iteration 4410: 0.05068\n",
            "Cost at iteration 4420: 0.05053\n",
            "Cost at iteration 4430: 0.05038\n",
            "Cost at iteration 4440: 0.05024\n",
            "Cost at iteration 4450: 0.05009\n",
            "Cost at iteration 4460: 0.04994\n",
            "Cost at iteration 4470: 0.04979\n",
            "Cost at iteration 4480: 0.04964\n",
            "Cost at iteration 4490: 0.04950\n",
            "Accuracy: 0.99 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "YBMDnvCmM5wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_preds = predict(updated_parameters, X_train)\n",
        "accuracy = score(y_train_preds, y_train)\n",
        "print(f'Accuracy on training set: {accuracy}')\n",
        "\n",
        "y_val_preds = predict(updated_parameters, X_val)\n",
        "accuracy = score(y_val_preds, y_val)\n",
        "print(f'Accuracy on validation set: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXzAnhC9kR6D",
        "outputId": "e34b7707-ddde-45ab-f947-97a58024d4f4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.9886591478696742\n",
            "Accuracy on validation set: 0.9638888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  The training set does slighly better than the validation set only 2% which i think the model is not really overfitting\n",
        "*  The Neural Networks model perform really well on recognizing image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v32JfVTmNAP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prediction(index, updated_parameters, X, y): # Plot to see the result on validation set\n",
        "\n",
        "    image = X[:, index, None]\n",
        "    prediction = predict(updated_parameters, image)\n",
        "    label = y[:, index, None]\n",
        "\n",
        "    print(f'Prediction: {prediction}')\n",
        "    print(f'Label: {label}')\n",
        "\n",
        "    image = image.reshape((28, 28)) * 255\n",
        "\n",
        "    plt.gray()\n",
        "    plt.imshow(image, interpolation ='nearest')\n",
        "    plt.title(f'Prediction: {prediction}, Label: {label}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_prediction(4544, updated_parameters, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "B3QoXas8xGoH",
        "outputId": "02184a6a-f639-4599-c7d0-752bf6dec480"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [2]\n",
            "Label: [[2]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "}# I create this cell only for submission\n",
        "predictions = predict(updated_parameters, test)\n",
        "\n",
        "col_name = ['Label']\n",
        "submission = pd.DataFrame(predictions, columns= col_name )\n",
        "\n",
        "submission.index = submission.index + 1\n",
        "submission.index.name = 'ImageId'\n",
        "submission.reset_index(inplace= True)\n",
        "\n",
        "submission.to_csv('/content/submit/submission.csv', index= False)"
      ],
      "metadata": {
        "id": "2MWV080iDoms"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AraRdrGeD6-U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}